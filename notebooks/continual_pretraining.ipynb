{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhkid/NeuroPurrfectAI-labs/blob/main/notebooks/continual_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Pretraining in Large Language Models - Concepts and Experiments\n",
        "\n",
        "This notebook demonstrates the practical implementation of continual pretraining (CPT)\n",
        "for language models. CPT enables models to be continuously updated with new knowledge\n",
        "without starting from scratch, addressing the challenge of static knowledge in LLMs."
      ],
      "metadata": {
        "id": "n_igo5OYWAYU"
      },
      "id": "n_igo5OYWAYU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Continual Pretraining?\n",
        "\n",
        "Continual pretraining allows language models to:\n",
        "- Adapt to new domains and data distributions\n",
        "- Incorporate fresh knowledge over time\n",
        "- Retain previously learned information (mitigating catastrophic forgetting)\n",
        "- Update efficiently without complete retraining\n",
        "\n",
        "There are two primary types of continual pretraining:\n",
        "1. **Continual general pre-training**: Updating the LLM with new data similar to original pre-training data\n",
        "2. **Continual domain-adaptive pre-training (DAP-training)**: Adapting the LLM to new domains\n",
        "\n",
        "In this notebook, we implement domain-adaptive continual pretraining using Parameter Isolation\n",
        "methods, specifically LoRA (Low-Rank Adaptation), to efficiently adapt a pretrained model\n",
        "to the cybersecurity domain.\n",
        "\n",
        "### Key Benefits of Continual Pretraining:\n",
        "- Better adaptation to domain-specific data\n",
        "- Cost and computational efficiency compared to full retraining\n",
        "- Reducing catastrophic forgetting using specialized techniques\n",
        "- Improved generalization to new, related tasks"
      ],
      "metadata": {
        "id": "_0q4PdeIWOL8"
      },
      "id": "_0q4PdeIWOL8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup\n",
        "\n",
        "## Setting Up the Environment\n",
        "\n",
        "First, we'll install the necessary libraries and authenticate with the Hugging Face Hub."
      ],
      "metadata": {
        "id": "CyF1MV2PXCzg"
      },
      "id": "CyF1MV2PXCzg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install triton==3.1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# Set up Hugging Face access token for downloading models\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "!huggingface-cli login --token $HF_TOKEN\n",
        "\n",
        "# Verify the authenticated user\n",
        "hf_user = !huggingface-cli whoami\n",
        "hf_user = hf_user[0]\n",
        "print(f\"Authenticated as: {hf_user}\")"
      ],
      "metadata": {
        "id": "5pZFf0cFV_zT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c44f4fb-247a-40b3-f625-3c41a7ade3b9"
      },
      "id": "5pZFf0cFV_zT",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `vietllm` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "Authenticated as: linhkid91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading The Model\n",
        "\n",
        "## Base Model Selection and Loading\n",
        "\n",
        "We'll use Gemma-3-1B with 4-bit quantization as our base model.\n",
        "This demonstrates the computational efficiency advantage of continual pretraining:\n",
        "- We start with a pretrained foundation model\n",
        "- We'll use parameter-efficient techniques (LoRA) to adapt it\n",
        "- 4-bit quantization reduces memory usage while maintaining performance"
      ],
      "metadata": {
        "id": "GFL-Yz1aYds7"
      },
      "id": "GFL-Yz1aYds7"
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 1024  # Maximum sequence length for training\n",
        "dtype = None  # Auto-detect precision (Float16 for Tesla T4/V100, Bfloat16 for Ampere+)\n",
        "load_in_4bit = True  # Use 4-bit quantization to reduce memory requirements\n",
        "\n",
        "# Load the pretrained model\n",
        "model_name = \"gemma-3-1b-pt\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = f\"unsloth/{model_name}-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Enable faster inference\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "plUNDn5xYfkb"
      },
      "id": "plUNDn5xYfkb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Isolation With LORA Adapters\n",
        "\n",
        "## Implementing Parameter Isolation with LoRA\n",
        "\n",
        "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that:\n",
        "- Adds small, trainable rank decomposition matrices to existing weights\n",
        "- Updates only a small subset of parameters (1-10%)\n",
        "- Preserves the original model's knowledge while learning new information\n",
        "\n",
        "This is an example of a Parameter Isolation method for continual learning, which:\n",
        "- Allocates separate parameters for new knowledge\n",
        "- Prevents catastrophic forgetting by not directly modifying original weights\n",
        "- Enables efficient adaptation with minimal computational resources\n",
        "\n",
        "For continual pretraining, we include embed_tokens and lm_head in the target modules to better learn out-of-distribution data from the new domain."
      ],
      "metadata": {
        "id": "dRUUrhLuYk8l"
      },
      "id": "dRUUrhLuYk8l"
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128,  # Rank of the update matrices. Higher rank = more capacity but more parameters\n",
        "    target_modules = [\n",
        "        # Attention modules\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        # MLP modules\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        # Token embedding and output head - critical for continual pretraining\n",
        "        \"embed_tokens\", \"lm_head\",\n",
        "    ],\n",
        "    lora_alpha = 32,  # Scaling factor for the LoRA updates\n",
        "    lora_dropout = 0,  # Dropout probability for LoRA layers (0 is optimized)\n",
        "    bias = \"none\",  # No bias parameters are trained\n",
        "    use_gradient_checkpointing = \"unsloth\",  # \"unsloth\" uses 30% less VRAM than standard gradient checkpointing\n",
        "    random_state = 3407,  # For reproducibility\n",
        "    use_rslora = True,  # Rank-stabilized LoRA for better training stability\n",
        "    loftq_config = None,  # No LoftQ quantization\n",
        ")"
      ],
      "metadata": {
        "id": "XoLjXvZ3Yu3O"
      },
      "id": "XoLjXvZ3Yu3O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Model Assesment\n",
        "\n",
        "## Evaluating the Model Before Continual Pretraining\n",
        "\n",
        "Let's test the model's response to a cybersecurity-related question before\n",
        "we perform continual pretraining. This will help us compare responses before\n",
        "and after adaptation to the cybersecurity domain."
      ],
      "metadata": {
        "id": "DQFOzuuwYxd3"
      },
      "id": "DQFOzuuwYxd3"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "import textwrap\n",
        "\n",
        "# Create a streamer for generating text\n",
        "text_streamer = TextIteratorStreamer(tokenizer)\n",
        "max_print_width = 100\n",
        "\n",
        "# Prepare the input query about cybersecurity\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    \"\"\"\n",
        "    what is penetration testing?\n",
        "\"\"\"\n",
        "], return_tensors = \"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "fX1wr24rY3Eq"
      },
      "id": "fX1wr24rY3Eq",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a response\n",
        "print(\"======== RESPONSE BEFORE CONTINUAL PRETRAINING ========\")\n",
        "outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64)\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "print(\"======================================================\")"
      ],
      "metadata": {
        "id": "6MfeIN0bY7ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8863cf73-884b-42e9-a6cc-8309f296c8c7"
      },
      "id": "6MfeIN0bY7ci",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== RESPONSE BEFORE CONTINUAL PRETRAINING ========\n",
            "<bos>\n",
            "    what is penetration testing?\n",
            "    penetration testing is a process that is used to test the security of a system.\n",
            "    penetration testing is used to test the security of a system.\n",
            "    penetration testing is used to test the security of a system.\n",
            "    penetration testing is used to test the security of a system.\n",
            "\n",
            "======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "## Preparing the Cybersecurity Training Data\n",
        "\n",
        "For continual pretraining, we'll use a cybersecurity-focused dataset.\n",
        "This demonstrates the concept of Continual Domain-Adaptive Pretraining (DAP-training),\n",
        "where we adapt the model to a specific new domain while retaining its general capabilities.\n",
        "\n",
        "Key points in data preparation:\n",
        "1. Format the text data with appropriate special tokens\n",
        "2. Split into training and test sets\n",
        "3. Ensure data quality for effective adaptation"
      ],
      "metadata": {
        "id": "PcZEkBKXY8iW"
      },
      "id": "PcZEkBKXY8iW"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Get the special end-of-sequence token from the tokenizer\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "print(f\"End of sequence token: {EOS_TOKEN}\")\n",
        "\n",
        "# Function to add EOS token to each example\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Add end-of-sequence token to each text example for proper training.\"\"\"\n",
        "    return {\"text_custom\": [example + EOS_TOKEN for example in examples[\"text\"]]}\n"
      ],
      "metadata": {
        "id": "iqg_fdT-ZBNo"
      },
      "id": "iqg_fdT-ZBNo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cybersecurity dataset\n",
        "dataset = load_dataset(\"clydeiii/cybersecurity\", split = \"train\")\n",
        "print(f\"Original dataset size: {len(dataset)} examples\")\n",
        "\n",
        "# Process dataset to add EOS tokens\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n"
      ],
      "metadata": {
        "id": "KD1b259TZEIP"
      },
      "id": "KD1b259TZEIP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small training split (0.5% of data) to demonstrate quick adaptation\n",
        "# In a real scenario, you might use more data for better adaptation\n",
        "train_dataset = dataset.train_test_split(test_size = 0.995)[\"train\"]\n",
        "print(f\"Training dataset size: {len(train_dataset)} examples\")\n"
      ],
      "metadata": {
        "id": "G7bVLWBiZGMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff62339-b6e4-4535-99ad-668480d63e40"
      },
      "id": "G7bVLWBiZGMK",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 2824 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample from the dataset to understand the content\n",
        "if len(train_dataset) > 0:\n",
        "    print(\"\\nSample data entry (truncated):\")\n",
        "    sample_text = train_dataset[0][\"text_custom\"]\n",
        "    print(sample_text[:500] + \"...\" if len(sample_text) > 500 else sample_text)"
      ],
      "metadata": {
        "id": "HTTKMda2ZGOf"
      },
      "id": "HTTKMda2ZGOf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06LWXEFxZGQK"
      },
      "id": "06LWXEFxZGQK",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Configuration\n",
        "\n",
        "## Configuring the Continual Pretraining Process\n",
        "\n",
        "Now we configure the continual pretraining process using UnslothTrainer,\n",
        "which is optimized for LoRA and memory efficiency. We'll define:\n",
        "\n",
        "### Training Hyperparameters:\n",
        "- Batch size and gradient accumulation steps\n",
        "- Learning rates (with special rate for embeddings)\n",
        "- Training schedule and optimizer settings\n",
        "\n",
        "### Addressing Continual Learning Challenges:\n",
        "- Using a slower learning rate for embeddings to prevent catastrophic forgetting\n",
        "- Configuring warmup to stabilize early training\n",
        "- Memory optimization for efficient training"
      ],
      "metadata": {
        "id": "sWhhFycIZK8Y"
      },
      "id": "sWhhFycIZK8Y"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "import torch"
      ],
      "metadata": {
        "id": "b344s4SDZRcs"
      },
      "id": "b344s4SDZRcs",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the trainer with specialized configuration for continual pretraining\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text_custom\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 8,  # Number of processes for dataset processing\n",
        "    packing = True,  # Enable input packing for efficiency\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        # Batch size configuration\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 16,  # Accumulate gradients to simulate larger batch\n",
        "\n",
        "        # Training schedule\n",
        "        warmup_ratio = 0.1,  # Percentage of steps for learning rate warmup\n",
        "        num_train_epochs = 1,  # For demonstration - more epochs might be needed for full adaptation\n",
        "\n",
        "        # Learning rates\n",
        "        learning_rate = 5e-5,  # Main learning rate for most parameters\n",
        "        embedding_learning_rate = 5e-6,  # Slower learning rate for embeddings to prevent catastrophic forgetting\n",
        "\n",
        "        # Precision settings\n",
        "        fp16 = not is_bfloat16_supported(),  # Use fp16 if bfloat16 not available\n",
        "        bf16 = is_bfloat16_supported(),  # Use bfloat16 if available (better for newer GPUs)\n",
        "\n",
        "        # Other settings\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",  # Memory-efficient optimizer\n",
        "        weight_decay = 0.00,  # No weight decay to simplify training\n",
        "        lr_scheduler_type = \"cosine\",  # Cosine schedule for smoother learning\n",
        "        seed = 3407,  # For reproducibility\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\"  # Disable reporting to save resources\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "ROFOLvvNZRoH"
      },
      "id": "ROFOLvvNZRoH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Management\n",
        "\n",
        "## GPU Memory Management\n",
        "\n",
        "Managing GPU memory is crucial for continual pretraining of large language models.\n",
        "We'll monitor memory usage before and after training to understand the resource\n",
        "requirements of our approach.\n",
        "\n",
        "This section highlights the computational"
      ],
      "metadata": {
        "id": "-2lQTkZpZYSP"
      },
      "id": "-2lQTkZpZYSP"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from numba import cuda"
      ],
      "metadata": {
        "id": "JNQEgLCrZRqc"
      },
      "id": "JNQEgLCrZRqc",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clear GPU cache\n",
        "def free_gpu_cache():\n",
        "    \"\"\"Clear GPU cache and run garbage collection to free memory.\"\"\"\n",
        "    print(\"Clearing GPU cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Running Python GC.\")\n",
        "    gc.collect()\n",
        "    print(\"Reseting device\")\n",
        "    device = cuda.get_current_device()\n",
        "    device.reset()"
      ],
      "metadata": {
        "id": "If16AhraZRsy"
      },
      "id": "If16AhraZRsy",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display GPU specifications and initial memory usage\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved before training.\")"
      ],
      "metadata": {
        "id": "QwLlY5wuZiMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e6d848d-f9fe-4e99-cd9b-7126f881dded"
      },
      "id": "QwLlY5wuZiMN",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA L4. Max memory = 22.161 GB.\n",
            "1.846 GB of memory reserved before training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Execution\n",
        "\n",
        "## Executing Continual Pretraining\n",
        "\n",
        "Now we'll run the actual continual pretraining process and track performance metrics."
      ],
      "metadata": {
        "id": "mqFyrK2jZkOh"
      },
      "id": "mqFyrK2jZkOh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the continual pretraining process\n",
        "print(\"Starting continual pretraining...\")\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "bLCZSomQZkZD"
      },
      "id": "bLCZSomQZkZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display training statistics\n",
        "print(f\"Training loss: {trainer_stats.training_loss}\")"
      ],
      "metadata": {
        "id": "w7zbOkjkZkef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607c82bd-05fe-4f1f-87e0-ac9cf23d23d1"
      },
      "id": "w7zbOkjkZkef",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 4.454508035020395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze memory usage after training\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)"
      ],
      "metadata": {
        "id": "0JE0ne8wZkhS"
      },
      "id": "0JE0ne8wZkhS",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training Performance Metrics ===\")\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "ox1qVQ56ZyPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5038f63a-9325-4374-fe9c-e76df646e595"
      },
      "id": "ox1qVQ56ZyPu",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Performance Metrics ===\n",
            "1766.6656 seconds used for training.\n",
            "29.44 minutes used for training.\n",
            "Peak reserved memory = 2.523 GB.\n",
            "Peak reserved memory for training = 0.677 GB.\n",
            "Peak reserved memory % of max memory = 11.385 %.\n",
            "Peak reserved memory for training % of max memory = 3.055 %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation After Training\n",
        "\n",
        "## Evaluating the Continually Pretrained Model\n",
        "\n",
        "Now we'll test the model again after continual pretraining to see how its\n",
        "responses about cybersecurity have improved. This demonstrates the model's\n",
        "adaptation to the new domain while preserving its general capabilities.\n",
        "\n",
        "We'll use the same input as before to see the difference in response quality."
      ],
      "metadata": {
        "id": "lpqqx4Z5Z0Te"
      },
      "id": "lpqqx4Z5Z0Te"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response with the updated model\n",
        "print(\"\\n======== RESPONSE AFTER CONTINUAL PRETRAINING ========\")\n",
        "outputs_new = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "print(tokenizer.batch_decode(outputs_new)[0])\n",
        "print(\"=======================================================\")"
      ],
      "metadata": {
        "id": "LIXjAy6jZ6Vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b995920f-4949-46dc-fb1b-4837cf98aeb7"
      },
      "id": "LIXjAy6jZ6Vd",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== RESPONSE AFTER CONTINUAL PRETRAINING ========\n",
            "<bos>\n",
            "    what is penetration testing?\n",
            "    What is the difference between penetration testing and ethical hacking?<eos>\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a different prompt to test general knowledge retention\n",
        "general_prompt = tokenizer(\n",
        "[\n",
        "    \"\"\"\n",
        "    What is the capital of France?\n",
        "\"\"\"\n",
        "], return_tensors = \"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "FRuqJCgeZ6Xx"
      },
      "id": "FRuqJCgeZ6Xx",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n======== TESTING GENERAL KNOWLEDGE RETENTION ========\")\n",
        "general_outputs = model.generate(**general_prompt, streamer = text_streamer, max_new_tokens = 64)\n",
        "print(tokenizer.batch_decode(general_outputs)[0])\n",
        "print(\"=======================================================\")"
      ],
      "metadata": {
        "id": "H2__0GlhZ6aH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0cbb2a-f991-47b8-bc6e-7c5df436384c"
      },
      "id": "H2__0GlhZ6aH",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== TESTING GENERAL KNOWLEDGE RETENTION ========\n",
            "<bos>\n",
            "    What is the capital of France?\n",
            "    What is the capital of France?<eos>\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Persistence\n",
        "\n",
        "## Saving the Continually Pretrained Model\n",
        "\n",
        "After successful continual pretraining, we can save the model in various formats:\n",
        "\n",
        "### Saving Options:\n",
        "1. **LoRA adapters only**: Most space-efficient, requires base model at loading time\n",
        "   - Ideal when you have multiple domain adaptations of the same base model\n",
        "   \n",
        "2. **Merged model with quantization**: Complete model with adapters merged into weights\n",
        "   - Options include 4-bit and 8-bit quantization for deployment efficiency\n",
        "   \n",
        "3. **Full precision merged model**: Best quality but largest file size\n",
        "   - Suitable when inference performance is critical\n",
        "\n",
        "Choose the appropriate option based on your deployment requirements."
      ],
      "metadata": {
        "id": "7dmxJhDEaAHx"
      },
      "id": "7dmxJhDEaAHx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the appropriate lines to save the model as needed\n",
        "\n",
        "# Option 1: Save LoRA adapters only (most efficient)\n",
        "# model.save_pretrained(\"adapters_cybersecurity\")\n",
        "\n",
        "# Option 2: Save locally with 4-bit quantization (balanced)\n",
        "# model.save_pretrained_merged(f\"{model_name}-cybersecurity-4bit\", tokenizer, save_method = \"merged_4bit\")\n",
        "\n",
        "# Option 3: Save to Hugging Face Hub with 4-bit quantization\n",
        "# model.push_to_hub_merged(f\"{hf_user}/{model_name}-cybersecurity-bnb-4bit\", tokenizer, save_method = \"merged_4bit\")\n"
      ],
      "metadata": {
        "id": "IylL-ZewaE9Z"
      },
      "id": "IylL-ZewaE9Z",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion And Key Takeaways\n",
        "\n",
        "This notebook demonstrated a practical implementation of continual pretraining using:\n",
        "1. A parameter isolation method (LoRA) to efficiently adapt a model\n",
        "2. Domain-specific data (cybersecurity) for targeted knowledge acquisition\n",
        "3. Techniques to manage resources and evaluate results\n",
        "\n",
        "### Key Benefits Demonstrated:\n",
        "- **Efficient adaptation** with minimal computational resources\n",
        "- **Preservation of general capabilities** with domain-specific enhancement\n",
        "- **Practical approach** to keeping models updated with new knowledge\n",
        "\n",
        "### Addressing Continual Learning Challenges:\n",
        "- **Catastrophic forgetting**: Mitigated through parameter isolation\n",
        "- **Computational efficiency**: Achieved through quantization and LoRA\n",
        "- **Domain adaptation**: Successfully implemented for cybersecurity knowledge\n",
        "\n",
        "### Future Extensions:\n",
        "- Experiment with different continual learning techniques (replay, regularization)\n",
        "- Test on multiple sequential domains to assess knowledge accumulation\n",
        "- Develop robust evaluation frameworks for continual pretraining\n",
        "- Explore other parameter-efficient methods like Adapters or Prefix-tuning"
      ],
      "metadata": {
        "id": "eztsFuH0aIKs"
      },
      "id": "eztsFuH0aIKs"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}