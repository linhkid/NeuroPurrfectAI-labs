{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Pretraining in Large Language Models - Concepts and Experiments\n",
        "\n",
        "This notebook demonstrates the practical implementation of continual pretraining (CPT)\n",
        "for language models. CPT enables models to be continuously updated with new knowledge\n",
        "without starting from scratch, addressing the challenge of static knowledge in LLMs."
      ],
      "metadata": {
        "id": "n_igo5OYWAYU"
      },
      "id": "n_igo5OYWAYU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Continual Pretraining?\n",
        "\n",
        "Continual pretraining allows language models to:\n",
        "- Adapt to new domains and data distributions\n",
        "- Incorporate fresh knowledge over time\n",
        "- Retain previously learned information (mitigating catastrophic forgetting)\n",
        "- Update efficiently without complete retraining\n",
        "\n",
        "There are two primary types of continual pretraining:\n",
        "1. **Continual general pre-training**: Updating the LLM with new data similar to original pre-training data\n",
        "2. **Continual domain-adaptive pre-training (DAP-training)**: Adapting the LLM to new domains\n",
        "\n",
        "In this notebook, we implement domain-adaptive continual pretraining using Parameter Isolation\n",
        "methods, specifically LoRA (Low-Rank Adaptation), to efficiently adapt a pretrained model\n",
        "to the cybersecurity domain.\n",
        "\n",
        "### Key Benefits of Continual Pretraining:\n",
        "- Better adaptation to domain-specific data\n",
        "- Cost and computational efficiency compared to full retraining\n",
        "- Reducing catastrophic forgetting using specialized techniques\n",
        "- Improved generalization to new, related tasks"
      ],
      "metadata": {
        "id": "_0q4PdeIWOL8"
      },
      "id": "_0q4PdeIWOL8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup\n",
        "\n",
        "## Setting Up the Environment\n",
        "\n",
        "First, we'll install the necessary libraries and authenticate with the Hugging Face Hub."
      ],
      "metadata": {
        "id": "CyF1MV2PXCzg"
      },
      "id": "CyF1MV2PXCzg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install triton==3.1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Hugging Face access token for downloading models\n",
        "os.environ['HF_TOKEN'] = \"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\"\n",
        "!huggingface-cli login --token $HF_TOKEN\n",
        "\n",
        "# Verify the authenticated user\n",
        "hf_user = !huggingface-cli whoami\n",
        "hf_user = hf_user[0]\n",
        "print(f\"Authenticated as: {hf_user}\")"
      ],
      "metadata": {
        "id": "5pZFf0cFV_zT"
      },
      "id": "5pZFf0cFV_zT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}